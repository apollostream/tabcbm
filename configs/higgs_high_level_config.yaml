trials: 5
results_dir: "results/higgs/high_level"
dataset: "higgs"
data_hyperparams:
    include_high_level: True
shared_params:
    # Experiment setup
    verbosity: 1
    aggr_key: "{model}"
    extra_name: ''
    continuous_concepts: True

    # Model configuration
    encoder_units: [1024, 512, 256, 128]
    decoder_units: [32, 16]
    latent_dims: 64

    # Misc Hyperparameters
    seed: 42
    eps: 1.0E-5

    # Training variables
    batch_size: 8192
    max_epochs: 1500
    learning_rate: 1.0E-3
    patience: 50
    min_delta: 1.0E-5
    holdout_fraction: 0.1
    early_stop_metric: "val_loss"
    early_stop_mode: "min"

    # Metrics variables
    cas_step: 10
    usable_concept_threshold: 0.85

runs:
    - model: 'TabCBM'
      pretrain_epochs: 25
      self_supervised_train_epochs: 25
      use_concept_embedding: False
      end_to_end_training: True
      coherence_reg_weight: 0.1
      diversity_reg_weight: "{{weight_used}}"
      feature_selection_reg_weight: "{{weight_used}}"
      contrastive_reg_weight: 0
      prob_diversity_reg_weight: 0
      gate_estimator_weight: 10
      n_exclusive_concepts: 0
      zero_mask: True
      normalized_scores: True
      temperature: 1
      top_k: 2048
      n_concepts: [4, 6, 8]
      aggr_key: "{model} (smaller k={n_concepts}, latent={latent_dims}, w={weight_used})"
      extra_name: "smaller_k_{n_concepts}_latent_{latent_dims}_weight_{weight_used}"
      encoder_units: [1024, 512, 256, 128]
      decoder_units: [32, 16]
      latent_dims: 64
      force_generator_inclusion: True
      weight_used: [0.01, 0.1]
      include_bn: True
      lr_schedule_decay: True
      grid_variables:
          - n_concepts
          - weight_used
      grid_search_mode: exhaustive
  
    - model: 'TabNet'
      pretrain_epochs: [0, 50]
      n_d: [96, 32]
      n_a: 32
      n_steps: [8, 5] #8
      gamma: 2.0
      momentum: 0.9
      lambda_sparse: 0.000001
      initial_lr: 0.025
      decay_rate: 0.9
      virtual_batch_size: 8192
      decay_step_size: 10
      pretraining_ratio: 0.8
      eps: 1.0E-15
      aggr_key: ["{model} large", "{model} small"]
      extra_name: ['large', '']
      grid_variables:
          - n_d
          - n_steps
          - pretrain_epochs
          - aggr_key
          - extra_name
      grid_search_mode: paired

    - model: 'MLP'
      latent_act: 'relu'
      include_bn: True
      encoder_units: [1024, 512, 256, 128]
      decoder_units: [32, 16]
      latent_dims: 64

    - model: 'XGBoost'
      extra_name: 'max_depth_{max_depth}_epochs_{max_epochs}'
      aggr_key: "{model} (d={max_depth}, epochs={max_epochs})"
      nthread: 6
      learning_rate: 1.0E-2
      max_depth: [5, 10]
      grid_variables:
          - max_depth
      grid_search_mode: exhaustive
  
    - model: 'LightGBM'
      extra_name: 'max_depth_{max_depth}_epochs_{max_epochs}'
      aggr_key: "{model} (d={max_depth}, epochs={max_epochs})"
      nthread: 6
      learning_rate: 1.0E-2
      max_depth: [5, 10]
      num_leaves: [32, 1024]
      verbosity: -1
      grid_variables:
          - max_depth
          - num_leaves
      grid_search_mode: paired
    
    - model: 'SENN'
      pretrain_autoencoder_epochs: 50
      regularization_strength: [0.1] #[0.01, 0.1]
      sparsity_strength: [2.0E-5] #[2.0E-5, 2.0E-3]
      coefficient_model_units: [300, 300]
      n_concepts: [8, 6, 4]
      aggr_key: "{model} (k={n_concepts}, pre_train={pretrain_autoencoder_epochs}, reg={regularization_strength}, sparsity={sparsity_strength})"
      extra_name: "k_{n_concepts}_pre_train_{pretrain_autoencoder_epochs}_reg_{regularization_strength}_sparsity_{sparsity_strength}"
      grid_variables:
          - n_concepts
          - regularization_strength
          - sparsity_strength
      grid_search_mode: exhaustive
      
    - model: 'CCD'
      pretrain_epochs: 150
      include_bn: True
      encoder_units: [1024, 512, 256, 128]
      decoder_units: [32, 16]
      latent_dims: 64
      n_concepts: [8, 6, 4]
      aggr_key: "{model} (k={n_concepts}, lambda1={lambda1}, lambda2={lambda2})"
      extra_name: "k_{n_concepts}_lambda1_{lambda1}__lambda2_{lambda2}"
      lambda1: [0.1] #[0.1, 1]
      lambda2: [0.1] #[0.1, 1]
      top_k: 2048
      threshold: 0.0
      grid_variables:
          - n_concepts
          - lambda1
          - lambda2
      grid_search_mode: exhaustive
  



