trials: 5
results_dir: "results/tabular_synth/nonlin_synth_complex"
dataset: "synth_tab_nonlinear_large"
shared_params:
    # Experiment setup
    verbosity: 0
    aggr_key: "{model}"
    extra_name: ''

    # Model configuration
    encoder_units: [16, 16]
    decoder_units: [16]
    latent_dims: 16

    # Misc Hyperparameters
    seed: 42
    eps: 1.0E-5

    # Training variables
    batch_size: 1024
    max_epochs: 1500
    learning_rate: 1.0E-3
    patience: 25
    min_delta: 1.0E-5
    holdout_fraction: 0.1
    early_stop_metric: "val_loss"
    early_stop_mode: "min"

    # Metrics variables
    cas_step: 10
    usable_concept_threshold: [0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.60, 0.65, 0.7, 0.75,0.8, 0.85]

runs:
    - model: 'PCA'
      n_concepts: "{{n_ground_truth_concepts}}"

    - model: 'TabTransformer'
      mlp_hidden_mults: [16, 16, 16]
      patience: 250

    - model: 'CEM'
      include_bn: True
      encoder_units: [16, 16]
      n_latent_acts: 16
      emb_size: 8
      concept_loss_weight: [0.1, 1, 5]
      n_supervised_concepts: [1, 2, 3, 4, 5]
      concept_supervision_annotated_fraction: [0.25, 0.5, 0.75, 1]
      n_concepts: "{{n_supervised_concepts}}"
      aggr_key: "{model} (k={n_concepts}, supervised_c={n_supervised_concepts}, annotated={concept_supervision_annotated_fraction}, c_weight={concept_loss_weight})"
      extra_name: "k_{n_concepts}_n_supervised_concepts_{n_supervised_concepts}_annotated_{concept_supervision_annotated_fraction}_c_weight_{concept_loss_weight}"
      patience: 250
      learning_rate: 0.01
      weight_decay: 4.0E-5
      grid_variables:
          - n_supervised_concepts
          - concept_supervision_annotated_fraction
          - concept_loss_weight
      grid_search_mode: exhaustive

    - model: 'CBM'
      pretrain_epochs: 100
      include_bn: True
      pass_concept_logits: False
      concept_loss_weight: [0.1, 1, 5]
      n_supervised_concepts: [1, 2, 3, 4, 5]
      concept_supervision_annotated_fraction: [0.25, 0.5, 0.75, 1]
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} (k={n_concepts}, supervised_c={n_supervised_concepts}, annotated={concept_supervision_annotated_fraction}, c_weight={concept_loss_weight})"
      extra_name: "k_{n_concepts}_n_supervised_concepts_{n_supervised_concepts}_annotated_{concept_supervision_annotated_fraction}_c_weight_{concept_loss_weight}"
      patience: 250
      grid_variables:
          - n_supervised_concepts
          - concept_supervision_annotated_fraction
          - concept_loss_weight
      grid_search_mode: exhaustive


    - model: 'CBM'
      pretrain_epochs: 100
      include_bn: True
      pass_concept_logits: False
      concept_loss_weight: [0.1, 1, 5]
      n_supervised_concepts: [1, 2, 3, 4, 5]
      concept_supervision_annotated_fraction: [0.25, 0.5, 0.75, 1]
      n_concepts: "{{n_supervised_concepts}}"
      aggr_key: "vanilla {model} (k={n_concepts}, supervised_c={n_supervised_concepts}, annotated={concept_supervision_annotated_fraction}, c_weight={concept_loss_weight})"
      extra_name: "vanilla_k_{n_concepts}_n_supervised_concepts_{n_supervised_concepts}_annotated_{concept_supervision_annotated_fraction}_c_weight_{concept_loss_weight}"
      patience: 250
      grid_variables:
          - n_supervised_concepts
          - concept_supervision_annotated_fraction
          - concept_loss_weight
      grid_search_mode: exhaustive

    - model: 'TabCBM'
      pretrain_epochs: 50
      self_supervised_train_epochs: 50
      use_concept_embedding: False
      end_to_end_training: True
      coherence_reg_weight: 0.1
      diversity_reg_weight: 5
      feature_selection_reg_weight: 5
      contrastive_reg_weight: 0
      prob_diversity_reg_weight: 0
      gate_estimator_weight: 10
      n_exclusive_concepts: 0
      zero_mask: True
      normalized_scores: True
      temperature: 1
      top_k: 256
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} (k={n_concepts})"
      extra_name: "k_{n_concepts}"
      early_stop_metric: "val_accuracy"
      early_stop_mode: "max"
      early_stop_metric_pretrain: "val_loss"
      early_stop_mode_pretrain: "min"
      patience: .inf


    - model: 'TabCBM'
      pretrain_epochs: 50
      self_supervised_train_epochs: 50
      use_concept_embedding: False
      end_to_end_training: True
      coherence_reg_weight: 0.1
      diversity_reg_weight: 5
      feature_selection_reg_weight: 5
      contrastive_reg_weight: 0
      prob_diversity_reg_weight: 0
      gate_estimator_weight: 10
      n_exclusive_concepts: 0
      zero_mask: True
      normalized_scores: True
      temperature: 1
      top_k: 256
      n_concepts: [3, 5, 10]
      aggr_key: "{model} (k={n_concepts})"
      extra_name: "k_{n_concepts}"
      early_stop_metric: "val_accuracy"
      early_stop_mode: "max"
      early_stop_metric_pretrain: "val_loss"
      early_stop_mode_pretrain: "min"
      patience: .inf
      grid_variables:
          - n_concepts
      grid_search_mode: exhaustive

    - model: 'TabCBM'
      pretrain_epochs: 50
      self_supervised_train_epochs: 50
      use_concept_embedding: False
      end_to_end_training: True
      coherence_reg_weight: 0.1
      diversity_reg_weight: 5
      feature_selection_reg_weight: 5
      contrastive_reg_weight: 0
      prob_diversity_reg_weight: 0
      gate_estimator_weight: 10
      n_exclusive_concepts: 0
      zero_mask: True
      normalized_scores: True
      temperature: 1
      top_k: 256
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} (k={n_concepts}, supervised_c={n_supervised_concepts}, annotated={concept_supervision_annotated_fraction}, concept_weight={concept_prediction_weight})"
      extra_name: "k_{n_concepts}_n_supervised_concepts_{n_supervised_concepts}_annotated_{concept_supervision_annotated_fraction}_concept_prediction_weight_{concept_prediction_weight}"
      early_stop_metric: "val_accuracy"
      early_stop_mode: "max"
      early_stop_metric_pretrain: "val_loss"
      early_stop_mode_pretrain: "min"
      patience: .inf
      n_supervised_concepts: [1, 2, 3, 4, 5]
      concept_supervision_annotated_fraction: [0.25, 0.5, 0.75, 1]
      concept_prediction_weight: [0.01, 0.1, 1, 5, 10, 100]
      grid_variables:
          - n_supervised_concepts
          - concept_supervision_annotated_fraction
          - concept_prediction_weight
      grid_search_mode: exhaustive



    - model: 'TabNet'
      pretrain_epochs: 100
      n_d: 8
      n_a: 8
      n_steps: 3
      gamma: 1.3
      momentum: 0.02
      lambda_sparse: 1.0E-3
      initial_lr: 0.02
      decay_rate: 0.9
      virtual_batch_size: 512
      decay_step_size: 10
      pretraining_ratio: 0.8
      eps: 1.0E-15

    - model: 'TabNet'
      pretrain_epochs: 100
      n_d: 64
      n_a: 64
      n_steps: 5
      gamma: 1.5
      momentum: 0.02
      lambda_sparse: 1.0E-3
      initial_lr: 0.02
      decay_rate: 0.9
      virtual_batch_size: 512
      decay_step_size: 10
      pretraining_ratio: 0.8
      eps: 1.0E-15
      extra_name: 'large_long'
      aggr_key: "{model} Large & Long"
      patience: 150
      max_epochs: 1000

    - model: 'MLP'

    - model: 'MLP'
      aggr_key: "{model} Large"
      encoder_units: [64, 64, 64]
      decoder_units: [64, 64]
      extra_name: "large"
      mlp_extra_units: [16]

    - model: 'XGBoost'
      extra_name: 'max_depth_{max_depth}_epochs_{max_epochs}'
      aggr_key: "{model} (d={max_depth}, epochs={max_epochs})"
      nthread: 6
      learning_rate: 1.0E-2
      max_depth: [5, 10]
      grid_variables:
          - max_depth
      grid_search_mode: exhaustive

    - model: 'LightGBM'
      extra_name: 'max_depth_{max_depth}_epochs_{max_epochs}'
      aggr_key: "{model} (d={max_depth}, epochs={max_epochs})"
      nthread: 6
      learning_rate: 1.0E-2
      max_depth: [5, 10]
      num_leaves: [32, 1024]
      verbosity: -1
      grid_variables:
          - max_depth
          - num_leaves
      grid_search_mode: paired

    - model: 'SENN'
      pretrain_autoencoder_epochs: 100
      regularization_strength: 0.1
      sparsity_strength: 2.0E-5
      coefficient_model_units: [64, 64]
      encoder_units: [64, 64, 64]
      decoder_units: [64, 64]
      latent_dims: 64
      patience: 50
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} Large (k={n_concepts})"
      extra_name: "_large_k_{n_concepts}"
      early_stop_metric: "val_sparse_top_k_categorical_accuracy"
      early_stop_mode: "max"

    - model: 'SENN'
      pretrain_autoencoder_epochs: 100
      regularization_strength: 0.1
      sparsity_strength: 2.0E-5
      coefficient_model_units: [16, 16]
      patience: 50
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} (k={n_concepts})"
      extra_name: "k_{n_concepts}"
      early_stop_metric: "val_sparse_top_k_categorical_accuracy"
      early_stop_mode: "max"

    - model: 'CCD'
      pretrain_epochs: 150
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} (k={n_concepts})"
      extra_name: "k_{n_concepts}"
      lambda1: 0.1
      lambda2: 0.1
      top_k: 256
      threshold: 0.0
      early_stop_metric: "val_sparse_categorical_accuracy"
      early_stop_mode: "max"
      patience: 50

    - model: 'CCD'
      pretrain_epochs: 150
      n_concepts: "{{n_ground_truth_concepts}}"
      aggr_key: "{model} Large (k={n_concepts})"
      extra_name: "large_k_{n_concepts}"
      encoder_units: [64, 64, 64]
      decoder_units: [64, 64]
      latent_dims: 64
      lambda1: 0.1
      lambda2: 0.1
      top_k: 256
      threshold: 0.0
      early_stop_metric: "val_sparse_categorical_accuracy"
      early_stop_mode: "max"
      patience: 50
