trials: 5
results_dir: "results/pbmc"
dataset: "pbmc"
shared_params:
    # Experiment setup
    verbosity: 1
    aggr_key: "{model}"
    extra_name: ''
    continuous_concepts: True

    # Model configuration
    encoder_units: [128, 64, 32]
    decoder_units: [16, 16]
    n_concepts: [3, 4, 5, 6]
    latent_dims: 8

    # Misc Hyperparameters
    seed: 42
    eps: 1.0E-5

    # Training variables
    batch_size: 1024
    virtual_batch_size: 1024
    top_k: null
    max_epochs: 1500
    learning_rate: 1.0E-3
    patience: 250
    min_delta: 1.0E-5
    holdout_fraction: 0.2
    early_stop_metric: "val_loss"
    early_stop_mode: "min"

    # Metrics variables
    cas_step: 10
    usable_concept_threshold: 0.85

runs:
    - model: 'TabTransformer'
      mlp_hidden_mults: [16, 16, 16, 16, 16]

    - model: 'TabCBM'
      early_stop_metric: "val_sparse_top_k_categorical_accuracy"
      early_stop_mode: "max"
      pretrain_epochs: 25
      self_supervised_train_epochs: 25
      coherence_reg_weight: 0.1
      diversity_reg_weight: "{{weight_used}}"
      feature_selection_reg_weight: "{{weight_used}}"
      gate_estimator_weight: 10

      aggr_key: "{model} (smaller k={n_concepts}, latent={latent_dims}, w={weight_used})"
      extra_name: "smaller_k_{n_concepts}_latent_{latent_dims}_weight_{weight_used}"
      weight_used: [0.1, 0.01]
      include_bn: True
      lr_schedule_decay: True
      concept_generator_units: [32, 16]
      rec_model_units: [16]
      grid_variables:
          - n_concepts
          - weight_used
      grid_search_mode: exhaustive


    - model: 'TabNet'
      pretrain_epochs: [0]
      n_d: [16]
      n_a: 16
      n_steps: [3]
      gamma: 2.0
      momentum: 0.9
      lambda_sparse: 0.000001
      initial_lr: 0.025
      decay_rate: 0.9
      decay_step_size: 10
      pretraining_ratio: 0.8
      eps: 1.0E-15
      aggr_key: ["{model}"]
      extra_name: ['']
      grid_variables:
          - n_d
          - n_steps
          - pretrain_epochs
          - aggr_key
          - extra_name
      grid_search_mode: paired

    - model: 'MLP'
      latent_act: 'relu'
      include_bn: True
      latent_dims: 64

    - model: 'XGBoost'
      extra_name: 'max_depth_{max_depth}_epochs_{max_epochs}'
      aggr_key: "{model} (d={max_depth}, epochs={max_epochs})"
      nthread: 6
      learning_rate: 1.0E-2
      max_depth: [5, 10]
      grid_variables:
          - max_depth
      grid_search_mode: exhaustive

    - model: 'LightGBM'
      extra_name: 'max_depth_{max_depth}_epochs_{max_epochs}'
      aggr_key: "{model} (d={max_depth}, epochs={max_epochs})"
      nthread: 6
      learning_rate: 1.0E-2
      max_depth: [5, 10]
      num_leaves: [32, 1024]
      verbosity: -1
      grid_variables:
          - max_depth
          - num_leaves
      grid_search_mode: paired

    - model: 'SENN'
      pretrain_autoencoder_epochs: 50
      regularization_strength: [0.01, 0.1]
      sparsity_strength: [2.0E-5, 2.0E-3]
      coefficient_model_units: [300, 300]
      aggr_key: "{model} (k={n_concepts}, pre_train={pretrain_autoencoder_epochs}, reg={regularization_strength}, sparsity={sparsity_strength})"
      extra_name: "k_{n_concepts}_pre_train_{pretrain_autoencoder_epochs}_reg_{regularization_strength}_sparsity_{sparsity_strength}"
      grid_variables:
          - n_concepts
          - regularization_strength
          - sparsity_strength
      grid_search_mode: exhaustive

    - model: 'CCD'
      pretrain_epochs: 150
      include_bn: True
      n_concepts: [8, 6, 4]
      aggr_key: "{model} (k={n_concepts}, lambda1={lambda1}, lambda2={lambda2})"
      extra_name: "k_{n_concepts}_lambda1_{lambda1}__lambda2_{lambda2}"
      lambda1: [0.1, 1]
      lambda2: [0.1, 1]
      threshold: 0.0
      grid_variables:
          - n_concepts
          - lambda1
          - lambda2
      grid_search_mode: exhaustive





