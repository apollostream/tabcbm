trials: 3
results_dir: "results/tabular_synth/ablation_nonlin_synth_complex"
dataset: "synth_tab_nonlinear_large"
shared_params:
    # Experiment setup
    verbosity: 0
    aggr_key: "{model}"
    extra_name: ''

    # Model configuration
    encoder_units: [16, 16]
    decoder_units: [16]
    latent_dims: 16

    # Misc Hyperparameters
    seed: 42
    eps: 1.0E-5

    # Training variables
    batch_size: 1024
    max_epochs: 1500
    learning_rate: 1.0E-3
    patience: 25
    min_delta: 1.0E-5
    holdout_fraction: 0.2
    early_stop_metric: "val_loss"
    early_stop_mode: "min"

    # Metrics variables
    cas_step: 10
    usable_concept_threshold: 0.85

runs:
    - model: 'TabCBM'
      pretrain_epochs: [0, 50]
      self_supervised_train_epochs: [0, 50]
      coherence_reg_weight: [0, 0.1, 5]
      diversity_reg_weight: [0, 0.1, 5]
      feature_selection_reg_weight: [0, 0.1, 5]
      gate_estimator_weight: 10
      top_k: 256
      n_concepts: [3, 5, 7]
      aggr_key: "{model} (k={n_concepts}, pretrain_epochs={pretrain_epochs}, self_supervised_train_epochs={self_supervised_train_epochs}, coherence_reg_weight={coherence_reg_weight}, diversity_reg_weight={diversity_reg_weight}, feature_selection_reg_weight={feature_selection_reg_weight})"
      extra_name: "k_{n_concepts}_pretrain_epochs_{pretrain_epochs}_self_supervised_train_epochs_{self_supervised_train_epochs}_coherence_reg_weight_{coherence_reg_weight}_diversity_reg_weight_{diversity_reg_weight}_feature_selection_reg_weight_{feature_selection_reg_weight}"
      early_stop_metric: "val_accuracy"
      early_stop_mode: "max"
      early_stop_metric_pretrain: "val_loss"
      early_stop_mode_pretrain: "min"
      patience: .inf
      grid_variables:
          - n_concepts
          - pretrain_epochs
          - self_supervised_train_epochs
          - coherence_reg_weight
          - diversity_reg_weight
          - feature_selection_reg_weight
      grid_search_mode: exhaustive